{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Berk_emotion_classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMO2+HtmzrEvXIeP7Q3andz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ODHojb63TQYH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084160172,"user_tz":-540,"elapsed":993,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["#신영"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdsZ8nnlQcxY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1600084162936,"user_tz":-540,"elapsed":3737,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"9e44231d-388c-4a13-a77c-6343e00dfe66"},"source":["# Hugging Face의 트랜스포머 모델을 설치\n","!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n","Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fqg7ZhReQsMi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084164934,"user_tz":-540,"elapsed":5721,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"eE2GezXTQ52x","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084164938,"user_tz":-540,"elapsed":5712,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["df=pd.read_csv(\"test_data1.tsv\", sep=\"\\t\", header=None, encoding=\"UTF-8\", names=['label','document',])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqW2g687k0nA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084164939,"user_tz":-540,"elapsed":5704,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["#중복 제거\n","possible_labels = df.label.unique()\n","\n","label_dict = {}\n","for index, possible_label in enumerate(possible_labels):\n","    label_dict[possible_label] = index\n","\n","df['label'] = df.label.replace(label_dict)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJVSJyWYQ9-P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600084164940,"user_tz":-540,"elapsed":5695,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"052f21c3-66b7-453e-84dd-e78b6d15e2e3"},"source":["# 판다스로 훈련셋과 테스트셋 데이터 로드\n","train = df[:1100]\n","test = df[1100:]\n","\n","print(train.shape)\n","print(test.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(1100, 2)\n","(266, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q3iIgZs7Q-fh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1600084164941,"user_tz":-540,"elapsed":5671,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"e7cca4be-bf0c-450e-b73b-b01b65367139"},"source":["# 훈련셋의 앞부분 출력\n","train.head(10)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>document</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>오예 오늘 휴강이다!!!</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>사랑한다. 이 세상 하나뿐인 내 아내와 아들, 딸</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>오늘 감기에 걸렸다..</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>놀이공원 놀러왔다 신난다! &gt;_&lt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>아 외롭다..ㅜ</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>3</td>\n","      <td>와 진짜 억울하다 프로젝트 다 짰는데 날렸다.... 미친거 아냐?? 아...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>아싸 시험 100점 ^^</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2</td>\n","      <td>오늘 키우던 개가 죽었어...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>헐 벌써 4월이야? 시간이 너무 빠르다!</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>대박 길 걷다가 만원 주웠다 ㅋㅋㅋ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                    document\n","0      0                               오예 오늘 휴강이다!!!\n","1      1                 사랑한다. 이 세상 하나뿐인 내 아내와 아들, 딸\n","2      2                                오늘 감기에 걸렸다..\n","3      0                          놀이공원 놀러왔다 신난다! >_<\n","4      2                                    아 외롭다..ㅜ\n","5      3  와 진짜 억울하다 프로젝트 다 짰는데 날렸다.... 미친거 아냐?? 아...\n","6      0                               아싸 시험 100점 ^^\n","7      2                            오늘 키우던 개가 죽었어...\n","8      4                      헐 벌써 4월이야? 시간이 너무 빠르다!\n","9      0                         대박 길 걷다가 만원 주웠다 ㅋㅋㅋ"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"w50MJCYtRAGr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"ok","timestamp":1600084164941,"user_tz":-540,"elapsed":5659,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"719639a8-a535-4f63-bd47-9236af4be76d"},"source":["# 리뷰 문장 추출\n","sentences = train['document']\n","sentences[:10]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                 오예 오늘 휴강이다!!!\n","1                   사랑한다. 이 세상 하나뿐인 내 아내와 아들, 딸\n","2                                  오늘 감기에 걸렸다..\n","3                            놀이공원 놀러왔다 신난다! >_<\n","4                                      아 외롭다..ㅜ\n","5    와 진짜 억울하다 프로젝트 다 짰는데 날렸다.... 미친거 아냐?? 아...\n","6                                 아싸 시험 100점 ^^\n","7                              오늘 키우던 개가 죽었어...\n","8                        헐 벌써 4월이야? 시간이 너무 빠르다!\n","9                           대박 길 걷다가 만원 주웠다 ㅋㅋㅋ\n","Name: document, dtype: object"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"18VFGSyMRDWN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600084164942,"user_tz":-540,"elapsed":5650,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"ae10bbf5-720c-4069-a0a0-3376d1c7c260"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 오예 오늘 휴강이다!!! [SEP]',\n"," '[CLS] 사랑한다. 이 세상 하나뿐인 내 아내와 아들, 딸 [SEP]',\n"," '[CLS] 오늘 감기에 걸렸다.. [SEP]',\n"," '[CLS] 놀이공원 놀러왔다 신난다! >_< [SEP]',\n"," '[CLS] 아 외롭다..ㅜ [SEP]',\n"," '[CLS] 와 진짜 억울하다 프로젝트 다 짰는데 날렸다.... 미친거 아냐?? 아... [SEP]',\n"," '[CLS] 아싸 시험 100점 ^^ [SEP]',\n"," '[CLS] 오늘 키우던 개가 죽었어... [SEP]',\n"," '[CLS] 헐 벌써 4월이야? 시간이 너무 빠르다! [SEP]',\n"," '[CLS] 대박 길 걷다가 만원 주웠다 ㅋㅋㅋ [SEP]']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"QR-aMz19eiWH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1600084164944,"user_tz":-540,"elapsed":5641,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"be5793d6-d37a-43c6-d326-0af8a4f0ca31"},"source":["train['label'].value_counts()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    498\n","2    233\n","3    120\n","6    115\n","4     79\n","1     37\n","5     17\n","7      1\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"0cIDJgLJRFEG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084164945,"user_tz":-540,"elapsed":5632,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"8a8ccbf0-bfd3-4064-abc9-c6fa965969a2"},"source":["# 라벨 추출\n","labels = train['label'].values\n","labels"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, ..., 0, 0, 4])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"XYn9tP8uRGJE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600084165613,"user_tz":-540,"elapsed":6291,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"f4779a91-fd04-4e55-c5e1-5cb5c7dbec07"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[CLS] 오예 오늘 휴강이다!!! [SEP]\n","['[CLS]', '오', '##예', '오', '##늘', '휴', '##강', '##이다', '!', '!', '!', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-md9r8FiRH4T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"status":"ok","timestamp":1600084165614,"user_tz":-540,"elapsed":6282,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"b4c90f9a-de5f-4b27-df7f-807e6261054a"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 256\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   101,   9580,  96279,   9580, 118762,  10013,  47181,  11925,\n","          106,    106,    106,    102,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"UStVfFcYRJW4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600084165616,"user_tz":-540,"elapsed":6275,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"55f0e2a6-7cca-4d13-fc07-0ec8926890aa"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RcSdxKQDRL_E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600084166082,"user_tz":-540,"elapsed":6731,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"4df8311e-5567-47b5-b7e0-8a2c1ebed316"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=41, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=41, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([   101,   9580, 118762,   8985,  49212,   9004,  32537,   9685,  11903,\n","           106,    106,    106,   9113,  48446,  12965,   9360,  10739,   9594,\n","         16439, 106154,   9389,  12692,   9343, 118691,   9946,  14867,   9685,\n","        118632,  11903,    100,   9429, 118873,  11903,    106,    102,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.])\n","tensor([  101,  8996, 18392, 12092,  9954, 31720, 28188,  9666, 80331,  9316,\n","        10003, 14279,  9694, 24989, 10530, 19653,  9960, 48549,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0])\n","tensor(6)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xSNEPA_qSNHC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084166083,"user_tz":-540,"elapsed":6722,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQPbCHh5SPM2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"ok","timestamp":1600084166084,"user_tz":-540,"elapsed":6716,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"bdeb17b0-7bca-43a1-a9e8-c8104a284aa0"},"source":["# 리뷰 문장 추출\n","sentences = test['document']\n","sentences[:10]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1100                                           수고했어, 지훈아!\n","1101                         행복요정이라더니..행복(부수기)요정인가...?(환장\n","1102         ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ ㅁㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이것이 흥개님의 사랑법,,♡ \n","1103    까우와 까우맘이 좋아해 가끔 부산에 출장 갈 때마다한번도 잊지 않고 어묵을 산 곳인...\n","1104                     헐.. 메일 보내셨구나.. 메일 귀찮아서 안보냈어요 ㅋㅋㅋ\n","1105              ㅌㅌㅋㅋ몽이가일케기엽ㅂ습니다..자식자랑나라사랑..(저유령계정댓나바요ㅠ \n","1106                   헐 당연하죠😊🙏🙏 꼬옥 남겨줄테니 다음에도 찍어주기~~~^^🤙\n","1107                         헐 슬로건ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㄴㅋㄴㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n","1108    ㅠㅠㅠㅠㅠㅠ 아 개 넘한 ㅠㅠㅠㅠㅠㅠㅠㅠㅠ 헐 ㅠㅠㅠㅠㅠㅠ 아 진짜 넘해요,,,,,...\n","1109                            헉헉헉 대박!!! 박성진!! 깜짝 놀랐어😨😨 \n","Name: document, dtype: object"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"wz_qQ8JxSQey","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1600084166085,"user_tz":-540,"elapsed":6707,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"1b92f6ea-6063-4f7b-d238-2788ec3a58bc"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 수고했어, 지훈아! [SEP]',\n"," '[CLS] 행복요정이라더니..행복(부수기)요정인가...?(환장 [SEP]',\n"," '[CLS] ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ ㅁㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이것이 흥개님의 사랑법,,♡  [SEP]',\n"," '[CLS] 까우와 까우맘이 좋아해 가끔 부산에 출장 갈 때마다한번도 잊지 않고 어묵을 산 곳인데.. 이제는 이런 소소한 행복과 추억 마저 안녕이네... ㅜ.  [SEP]',\n"," '[CLS] 헐.. 메일 보내셨구나.. 메일 귀찮아서 안보냈어요 ㅋㅋㅋ [SEP]',\n"," '[CLS] ㅌㅌㅋㅋ몽이가일케기엽ㅂ습니다..자식자랑나라사랑..(저유령계정댓나바요ㅠ  [SEP]',\n"," '[CLS] 헐 당연하죠😊🙏🙏 꼬옥 남겨줄테니 다음에도 찍어주기~~~^^🤙 [SEP]',\n"," '[CLS] 헐 슬로건ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㄴㅋㄴㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ [SEP]',\n"," '[CLS] ㅠㅠㅠㅠㅠㅠ 아 개 넘한 ㅠㅠㅠㅠㅠㅠㅠㅠㅠ 헐 ㅠㅠㅠㅠㅠㅠ 아 진짜 넘해요,,,,,,ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ [SEP]',\n"," '[CLS] 헉헉헉 대박!!! 박성진!! 깜짝 놀랐어😨😨  [SEP]']"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"8Gl0ur-zSRVj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"status":"ok","timestamp":1600084166085,"user_tz":-540,"elapsed":6695,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"a0b85fef-93b5-44e5-c319-66b589e1bf3b"},"source":["# 라벨 추출\n","labels = test['label'].values\n","labels"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 2, 0, 0, 0, 0, 2, 4, 0, 2, 0, 1, 2, 2, 0, 0, 4, 0, 6, 4,\n","       2, 4, 3, 4, 4, 3, 2, 1, 2, 4, 1, 0, 0, 4, 0, 0, 0, 2, 4, 4, 2, 0,\n","       0, 0, 2, 0, 2, 0, 3, 2, 0, 0, 0, 4, 3, 1, 2, 3, 2, 0, 6, 0, 2, 0,\n","       3, 6, 0, 3, 3, 2, 6, 3, 0, 2, 0, 6, 1, 2, 3, 3, 2, 3, 6, 3, 0, 2,\n","       3, 0, 3, 2, 3, 3, 2, 2, 2, 3, 6, 4, 0, 0, 0, 0, 2, 4, 2, 6, 1, 2,\n","       0, 4, 4, 4, 2, 1, 1, 6, 0, 0, 2, 1, 0, 2, 6, 4, 6, 0, 0, 6, 6, 6,\n","       0, 0, 0, 0, 0, 0, 0, 5, 0, 2, 4, 2, 0, 2, 1, 4, 6, 6, 4, 4, 4, 1,\n","       6, 1, 6, 6, 2, 0, 0, 0, 6, 0, 4, 4, 4, 4, 2, 1, 2, 0, 0, 4, 1, 2,\n","       4, 2, 0, 3, 0, 0, 2, 0, 3, 2, 6, 0, 2, 0, 2, 0, 0, 4, 1, 5, 6, 0,\n","       6, 4, 3, 3, 6, 0, 2, 0, 2, 0, 6, 0, 3, 3, 0, 3, 2, 6, 2, 0, 2, 0,\n","       3, 0, 6, 0, 0, 3, 2, 2, 2, 3, 0, 0, 0, 2, 6, 2, 2, 2, 2, 0, 0, 0,\n","       2, 3, 6, 0, 6, 0, 3, 2, 0, 3, 6, 0, 2, 0, 0, 1, 5, 3, 3, 2, 0, 4,\n","       6, 3])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"_st0uoGKSS28","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600084166425,"user_tz":-540,"elapsed":7025,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"10ed994d-1143-48e0-f8b5-3a5eb9c13c5d"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[CLS] 수고했어, 지훈아! [SEP]\n","['[CLS]', '수', '##고', '##했', '##어', ',', '지', '##훈', '##아', '!', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KGBtMoriSUOU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1600084166427,"user_tz":-540,"elapsed":7017,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"7b5b1b65-7343-434f-e940-f5826bf49430"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   101,   9460,  11664, 119424,  12965,    117,   9706,  75965,\n","        16985,    106,    102,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"x8pDSI1rSVlO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1600084166428,"user_tz":-540,"elapsed":7008,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"31f79f0a-004e-4944-8ac9-848ff58a1abd"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bE7PFCCESWyq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"status":"ok","timestamp":1600084166429,"user_tz":-540,"elapsed":6998,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"ede92816-4412-4a2e-f9d0-00be38a77449"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["tensor([   101,   9460,  11664, 119424,  12965,    117,   9706,  75965,  16985,\n","           106,    102,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HTgIqO2jSYoC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084166431,"user_tz":-540,"elapsed":6990,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jk0l3-WeSar9","colab_type":"text"},"source":["모델 생성"]},{"cell_type":"code","metadata":{"id":"LUEkLs2sSbv4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084167294,"user_tz":-540,"elapsed":7846,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"31dc2407-bdd6-4c4d-ffda-6825f209513a"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5zt285RuSrqj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600084167296,"user_tz":-540,"elapsed":7839,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"bb35a4ad-3d89-4c3d-c660-dd4a3fa2e260"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"isxDW-RvStNV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600084177220,"user_tz":-540,"elapsed":17753,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"644cf4c3-bf30-4bba-ec68-523e6e2398d6"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(label_dict))\n","model.cuda()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"LKk1qy5zSwAF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084177221,"user_tz":-540,"elapsed":17745,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 학습률을 조금씩 감소시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bXi3W-ALTc6r","colab_type":"text"},"source":["모델 학습\n"]},{"cell_type":"code","metadata":{"id":"W_Ar-hy4SyhD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084177221,"user_tz":-540,"elapsed":17738,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"RX9oDjIKS1wE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084177222,"user_tz":-540,"elapsed":17733,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QGZdNf6S3NM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":756},"executionInfo":{"status":"ok","timestamp":1600084367045,"user_tz":-540,"elapsed":207550,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"02ac18b4-93a2-4c6a-da34-1918cedb6407"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 19\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":31,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 1.62\n","  Training epcoh took: 0:00:46\n","\n","Running Validation...\n","  Accuracy: 0.47\n","  Validation took: 0:00:02\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.43\n","  Training epcoh took: 0:00:45\n","\n","Running Validation...\n","  Accuracy: 0.54\n","  Validation took: 0:00:02\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 1.28\n","  Training epcoh took: 0:00:46\n","\n","Running Validation...\n","  Accuracy: 0.56\n","  Validation took: 0:00:02\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 1.16\n","  Training epcoh took: 0:00:45\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation took: 0:00:02\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"84s_uS8JTHDK","colab_type":"text"},"source":["테스트셋 평가"]},{"cell_type":"code","metadata":{"id":"ptxqyY5iTD1e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1600084369245,"user_tz":-540,"elapsed":209740,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"73ede12f-8c6c-4eb8-b71c-6714320caff4"},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["\n","Accuracy: 0.50\n","Test took: 0:00:02\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RzSkwqGnTLiG","colab_type":"text"},"source":["새로운 문장 테스트¶"]},{"cell_type":"code","metadata":{"id":"xOOD0xZRTEQI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084369246,"user_tz":-540,"elapsed":209729,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 256\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0R53ru8TO-r","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600084369247,"user_tz":-540,"elapsed":209722,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}}},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hc0cPreri5R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369247,"user_tz":-540,"elapsed":209715,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"421c33af-fc91-450b-d110-d2043d40f37a"},"source":["logits = test_sentences(['열심히 일하고 만족을 미루는 것이 내가 할 수 있는 전부다!'])\n","\n","husky=np.argmax(logits)\n","print(possible_labels[husky])\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["neutral\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6OUXfR68cAGF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369248,"user_tz":-540,"elapsed":209706,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"faed957d-3be7-4666-ba60-7038f854f1ad"},"source":["logits = test_sentences(['시작은 미약해도 그 끝은 창대할 수 있다.'])\n","\n","husky=np.argmax(logits)\n","print(possible_labels[husky])"],"execution_count":36,"outputs":[{"output_type":"stream","text":["sadness\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VpyWLqjXTRfc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369249,"user_tz":-540,"elapsed":209697,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"1bc5b25a-6635-462e-cedc-8c0ffaba4366"},"source":["\n","logits = test_sentences(['우리 팀의 팀훈(?) ❗️워라밸❗️을 위해 체계적으로 진행하려고 스크럼도 하고 그랬는데, 워라밸을 떠나서 매끄럽게 진행되고있다. 매우 행복하고 만족스럽다!\\(//∇//)\\\n","SNS(페이스북, 인스타그램 등) 데이터 가져오는 것이 매우 걱정했는데 성공했다. 이게 행복이지. 이맛에 코딩하는건가?(°▽°)❗️ 짜릿해 오늘 팀플의 기분은 행복함, 그리고 평화로움이다.'])\n","husky=np.argmax(logits)\n","print(possible_labels[husky])"],"execution_count":37,"outputs":[{"output_type":"stream","text":["joy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0A5zCULZ2GcP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369250,"user_tz":-540,"elapsed":209688,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"a844978b-aa32-4246-d542-ea27c5ec06ad"},"source":["logits = test_sentences(['갓난 아이 때 보육원에 와서 여섯 살 때 그 집으로 입양됐어요. 좋은 집에서 잘 먹고 편하게 살았지만 또 마냥 행복한 것만은 아니었어요. 그 때 홍콩엔 친엄마를 만나러 갔던 거예요. 죽기 전에 널 보고 싶다.. 전화가 왔었거든.. 날 만나고 그날 밤에 엄마는 돌아가셨어요. 끔찍했죠.. 엄마는 형편이 나아지는 대로 날 다시 데려가겠다 했는데 돈 많은 교수부부 집에 간 걸 알고 마음을 접으셨대요.대신 우유배달부로, 요구르트 아줌마로 늘 내 곁을 맴돌면서 사셨다고..비 오는 날 학교에서 우산을 건네줬던 요구르트 아줌마가 있었는데.. 엄마였대요.그 때 한번 손이라도 잡아볼 껄..'])\n","husky=np.argmax(logits)\n","print(possible_labels[husky])\n","\n","# sadness를 예상했으나 joy가.."],"execution_count":38,"outputs":[{"output_type":"stream","text":["joy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lVO4OlrgZEWX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369250,"user_tz":-540,"elapsed":209679,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"64887511-e0e4-4be7-a109-e07fb8427679"},"source":["logits = test_sentences(['난 당신하구 결혼하면서 내 인생을 송두리째 포기한거야! 거기다 대고 뭘 양보했냐고?'])\n","husky=np.argmax(logits)\n","print(possible_labels[husky])\n","\n","# 오.. 잘나왔다"],"execution_count":39,"outputs":[{"output_type":"stream","text":["anger\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WEQKZDTCZE3n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369251,"user_tz":-540,"elapsed":209670,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"3f612f30-746d-4f78-dcb3-0dc4c1961a8a"},"source":["logits = test_sentences(['아버지가가방에들어가신다'])\n","husky=np.argmax(logits)\n","print(possible_labels[husky])"],"execution_count":40,"outputs":[{"output_type":"stream","text":["joy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTx_drh4ZFrq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369252,"user_tz":-540,"elapsed":209661,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"1351b58f-7844-4098-b350-e5480a639b64"},"source":["logits = test_sentences(['철수는 울면서 떠나는 영희를 배웅했다.'])\n","husky=np.argmax(logits)\n","print(possible_labels[husky])\n","\n","# ..."],"execution_count":41,"outputs":[{"output_type":"stream","text":["joy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bNEa4ypRZG3R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600084369253,"user_tz":-540,"elapsed":209653,"user":{"displayName":"김신영","photoUrl":"","userId":"11478318935090756381"}},"outputId":"90981698-2fc2-4595-d7c7-f5045e89fd73"},"source":["logits = test_sentences(['하기야, 육지에서는 나를 몰라보는 이가 없소이다마는, 용궁에까지 소문이 났다 하니 조금은 놀랍소.'])\n","husky=np.argmax(logits)\n","print(possible_labels[husky])"],"execution_count":42,"outputs":[{"output_type":"stream","text":["anger\n"],"name":"stdout"}]}]}