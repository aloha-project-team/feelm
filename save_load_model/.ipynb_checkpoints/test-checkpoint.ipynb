{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.zeros((2,1))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.23.2-cp36-cp36m-win_amd64.whl (6.8 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from scikit-learn) (0.16.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.1.2-cp36-cp36m-win_amd64.whl (9.4 MB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.2 pytz-2020.1\n",
      "Requirement already satisfied: numpy in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from Keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from Keras) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from Keras) (1.18.5)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1-cp36-cp36m-win_amd64.whl (215 kB)\n",
      "Requirement already satisfied: six in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from h5py->Keras) (1.15.0)\n",
      "Installing collected packages: pyyaml, Keras\n",
      "Successfully installed Keras-2.4.3 pyyaml-5.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-3.1.0-py3-none-any.whl (884 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-win_amd64.whl (1.2 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.7.14-cp36-cp36m-win_amd64.whl (268 kB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (20.4)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "  Downloading tokenizers-0.8.1rc2-cp36-cp36m-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from packaging->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=feb58178d5ffc5ddef21c2d0d1dcd48fc65ac4b6aa65ccdc4ef71344f4bab57c\n",
      "  Stored in directory: c:\\users\\gmlrn\\appdata\\local\\pip\\cache\\wheels\\49\\25\\98\\cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: filelock, tqdm, sentencepiece, regex, dataclasses, click, joblib, sacremoses, tokenizers, transformers\n",
      "Successfully installed click-7.1.2 dataclasses-0.7 filelock-3.0.12 joblib-0.16.0 regex-2020.7.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 tqdm-4.49.0 transformers-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.3.0-cp36-cp36m-win_amd64.whl (342.5 MB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp36-cp36m-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Processing c:\\users\\gmlrn\\appdata\\local\\pip\\cache\\wheels\\93\\2a\\eb\\e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-win_amd64.whl (30.8 MB)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.6.0.post20200814)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.21.1-py2.py3-none-any.whl (93 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\gmlrn\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: gast, urllib3, chardet, idna, requests, tensorboard-plugin-wit, oauthlib, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, werkzeug, markdown, tensorboard, opt-einsum, keras-preprocessing, h5py, termcolor, scipy, tensorflow\n",
      "Successfully installed cachetools-4.1.1 chardet-3.0.4 gast-0.3.3 google-auth-1.21.1 google-auth-oauthlib-0.4.1 h5py-2.10.0 idna-2.10 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 termcolor-1.1.0 urllib3-1.25.10 werkzeug-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'love', 'sadness', 'anger', 'surprise', 'fear', 'neutral',\n",
       "       'anger Í≤ΩÌñ• Í∏∞Î†àÍ∏∞Ïùò ÌïúÌÉÑ. ÍπÄÏÉÅÏ°∞ ÏúÑÏû•Ï†ÑÏûÖ Í∏∞ÏÇ¨Î•º Ïì∏ÎïåÎßàÎã§ ÏöïÏùÑ Î∞îÍ∞ÄÏßÄÎ°ú Î®πÎäîÎã§. Í∏∞ÏûêÍ∞Ä Í∏∞Î†àÍ∏∞ ÏÜåÎ¶¨ Îì£Í∏∞ Ïã´Ïñ¥ÏÑú Ï∞©Ìïú Í∏∞ÏÇ¨Îßå Ïì∞Îã§Î≥¥Î©¥ ÏßÑÏßú Í∏∞Î†àÍ∏∞ Îê† Ïàò ÏûàÎã§. Í∏∞Î†àÍ∏∞Í∞Ä Í∏∞Î†àÍ∏∞ ÎêòÍ∏∞ Ïã´Ïñ¥ÏÑú Ïì∞Î†àÍ∏∞Í∞ôÏùÄ Í∏∞ÏÇ¨Îßå Ïì¥Îã§Í≥† Ï£ºÏû•...'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Desktop/AI_SCHOOL/WarmingUp/feelm/sentiment_analysis_bert/test_data.tsv\", sep=\"\\t\", header=None)\n",
    "df.columns = [\"sentiment\", \"text\"]\n",
    "\n",
    "df = df[[\"text\", \"sentiment\"]]\n",
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>NaN</td>\n",
       "      <td>anger Í≤ΩÌñ• Í∏∞Î†àÍ∏∞Ïùò ÌïúÌÉÑ. ÍπÄÏÉÅÏ°∞ ÏúÑÏû•Ï†ÑÏûÖ Í∏∞ÏÇ¨Î•º Ïì∏ÎïåÎßàÎã§ ÏöïÏùÑ Î∞îÍ∞ÄÏßÄÎ°ú Î®πÎäî...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text                                          sentiment\n",
       "552  NaN  anger Í≤ΩÌñ• Í∏∞Î†àÍ∏∞Ïùò ÌïúÌÉÑ. ÍπÄÏÉÅÏ°∞ ÏúÑÏû•Ï†ÑÏûÖ Í∏∞ÏÇ¨Î•º Ïì∏ÎïåÎßàÎã§ ÏöïÏùÑ Î∞îÍ∞ÄÏßÄÎ°ú Î®πÎäî..."
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"sentiment\"] == 'anger Í≤ΩÌñ• Í∏∞Î†àÍ∏∞Ïùò ÌïúÌÉÑ. ÍπÄÏÉÅÏ°∞ ÏúÑÏû•Ï†ÑÏûÖ Í∏∞ÏÇ¨Î•º Ïì∏ÎïåÎßàÎã§ ÏöïÏùÑ Î∞îÍ∞ÄÏßÄÎ°ú Î®πÎäîÎã§. Í∏∞ÏûêÍ∞Ä Í∏∞Î†àÍ∏∞ ÏÜåÎ¶¨ Îì£Í∏∞ Ïã´Ïñ¥ÏÑú Ï∞©Ìïú Í∏∞ÏÇ¨Îßå Ïì∞Îã§Î≥¥Î©¥ ÏßÑÏßú Í∏∞Î†àÍ∏∞ Îê† Ïàò ÏûàÎã§. Í∏∞Î†àÍ∏∞Í∞Ä Í∏∞Î†àÍ∏∞ ÎêòÍ∏∞ Ïã´Ïñ¥ÏÑú Ïì∞Î†àÍ∏∞Í∞ôÏùÄ Í∏∞ÏÇ¨Îßå Ïì¥Îã§Í≥† Ï£ºÏû•...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'love', 'sadness', 'anger', 'surprise', 'fear', 'neutral'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] Ïò§Ïòà Ïò§Îäò Ìú¥Í∞ïÏù¥Îã§!!! [SEP]',\n",
       " '[CLS] ÏÇ¨ÎûëÌïúÎã§. Ïù¥ ÏÑ∏ÏÉÅ ÌïòÎÇòÎøêÏù∏ ÎÇ¥ ÏïÑÎÇ¥ÏôÄ ÏïÑÎì§, Îî∏ [SEP]',\n",
       " '[CLS] Ïò§Îäò Í∞êÍ∏∞Ïóê Í±∏Î†∏Îã§.. [SEP]',\n",
       " '[CLS] ÎÜÄÏù¥Í≥µÏõê ÎÜÄÎü¨ÏôîÎã§ Ïã†ÎÇúÎã§! >_< [SEP]',\n",
       " '[CLS] ÏïÑ Ïô∏Î°≠Îã§..„Öú [SEP]',\n",
       " '[CLS] ÏôÄ ÏßÑÏßú ÏñµÏö∏ÌïòÎã§ ÌîÑÎ°úÏ†ùÌä∏ Îã§ Ïß∞ÎäîÎç∞ ÎÇ†Î†∏Îã§.... ÎØ∏ÏπúÍ±∞ ÏïÑÎÉê?? ÏïÑ... [SEP]',\n",
       " '[CLS] ÏïÑÏã∏ ÏãúÌóò 100Ï†ê ^^ [SEP]',\n",
       " '[CLS] Ïò§Îäò ÌÇ§Ïö∞Îçò Í∞úÍ∞Ä Ï£ΩÏóàÏñ¥... [SEP]',\n",
       " '[CLS] Ìóê Î≤åÏç® 4ÏõîÏù¥Ïïº? ÏãúÍ∞ÑÏù¥ ÎÑàÎ¨¥ Îπ†Î•¥Îã§! [SEP]',\n",
       " '[CLS] ÎåÄÎ∞ï Í∏∏ Í±∑Îã§Í∞Ä ÎßåÏõê Ï£ºÏõ†Îã§ „Öã„Öã„Öã [SEP]']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text\n",
    "text = df[\"text\"][:500]\n",
    "# BERTÏùò ÏûÖÎ†• ÌòïÏãùÏóê ÎßûÍ≤å Î≥ÄÌôò\n",
    "text = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in text]\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 5, 2, 5, 0, 2, 5, 6, 2, 5, 1, 2, 5, 6, 2, 5, 6, 2, 5, 2, 2,\n",
       "       5, 5, 2, 5, 6, 2, 2, 6, 2, 5, 3, 2, 2, 2, 6, 6, 2, 6, 5, 5, 2, 0,\n",
       "       2, 2, 2, 2, 2, 4, 4, 2, 6, 3, 4, 3, 5, 5, 0, 4, 4, 4, 0, 2, 2, 2,\n",
       "       6, 5, 5, 2, 2, 2, 4, 5, 2, 5, 5, 2, 5, 2, 5, 5, 2, 0, 2, 2, 5, 5,\n",
       "       2, 5, 5, 5, 3, 5, 0, 0, 5, 5, 2, 3, 4, 2, 4, 2, 5, 5, 2, 4, 0, 2,\n",
       "       4, 4, 4, 5, 2, 2, 4, 2, 5, 6, 2, 2, 6, 2, 0, 2, 0, 2, 2, 2, 2, 6,\n",
       "       0, 0, 6, 4, 2, 0, 0, 2, 3, 2, 5, 2, 0, 2, 2, 2, 2, 6, 5, 2, 2, 0,\n",
       "       5, 4, 5, 0, 5, 6, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 4, 2, 2, 2, 0, 5,\n",
       "       3, 2, 3, 5, 2, 5, 5, 0, 5, 2, 2, 5, 5, 0, 5, 5, 4, 2, 4, 4, 2, 3,\n",
       "       6, 4, 5, 5, 2, 2, 2, 5, 3, 2, 4, 4, 5, 4, 2, 2, 6, 5, 6, 4, 5, 2,\n",
       "       4, 2, 0, 5, 2, 0, 1, 0, 3, 2, 5, 2, 6, 2, 2, 5, 2, 5, 4, 2, 2, 5,\n",
       "       3, 5, 4, 4, 2, 2, 5, 2, 2, 5, 2, 3, 5, 5, 5, 6, 6, 2, 2, 2, 2, 3,\n",
       "       2, 5, 5, 2, 6, 2, 3, 0, 5, 3, 5, 2, 2, 2, 2, 4, 2, 2, 5, 5, 6, 4,\n",
       "       3, 0, 5, 5, 3, 6, 2, 2, 3, 2, 2, 5, 5, 2, 2, 6, 3, 5, 3, 2, 2, 2,\n",
       "       6, 2, 2, 2, 6, 3, 2, 5, 2, 6, 5, 5, 2, 3, 2, 6, 3, 5, 2, 2, 5, 2,\n",
       "       2, 0, 2, 2, 4, 4, 6, 4, 2, 5, 3, 2, 6, 2, 5, 2, 0, 2, 0, 5, 5, 2,\n",
       "       5, 5, 2, 2, 6, 0, 2, 2, 2, 2, 0, 0, 2, 4, 4, 2, 2, 2, 2, 2, 5, 2,\n",
       "       2, 3, 0, 5, 0, 2, 2, 5, 2, 2, 1, 0, 2, 6, 4, 2, 4, 2, 2, 0, 2, 2,\n",
       "       2, 5, 5, 2, 2, 0, 2, 2, 2, 5, 5, 2, 3, 2, 5, 2, 2, 4, 0, 2, 4, 2,\n",
       "       2, 2, 2, 4, 5, 2, 2, 0, 2, 2, 2, 2, 0, 2, 4, 6, 2, 2, 2, 2, 2, 2,\n",
       "       5, 5, 2, 6, 2, 6, 2, 2, 2, 2, 4, 2, 5, 0, 5, 2, 4, 6, 2, 5, 6, 0,\n",
       "       2, 2, 6, 2, 2, 2, 4, 4, 2, 6, 2, 5, 2, 2, 0, 4, 5, 4, 2, 2, 2, 4,\n",
       "       2, 2, 2, 2, 5, 5, 2, 4, 4, 2, 5, 5, 5, 5, 2, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labelÏù¥ Î¨∏ÏûêÏù¥ÎØÄÎ°ú Ïà´ÏûêÍ∞íÏúºÎ°ú Î∞îÍøîÏ§òÏïºÌï® - LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "sentiment = label_encoder.fit_transform(df[\"sentiment\"][:500])\n",
    "\n",
    "# label_encoder.classes_\n",
    "# label_encoder.inverse_transform(sentiment)\n",
    "\n",
    "# sentiment = sentiment.reshape(-1,1)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Ïò§Ïòà Ïò§Îäò Ìú¥Í∞ïÏù¥Îã§!!! [SEP]\n",
      "['[CLS]', 'Ïò§', '##Ïòà', 'Ïò§', '##Îäò', 'Ìú¥', '##Í∞ï', '##Ïù¥Îã§', '!', '!', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERTÏùò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Î¨∏Ïû•ÏùÑ ÌÜ†ÌÅ∞ÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in text]\n",
    "\n",
    "print (text[0])\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519, 100585,   9519, 118825, 118911,  23811,   9568,\n",
       "        52363,   9004,  32537,    100,  68409, 108578,  12092,   9309,\n",
       "        55358,   9568,  12310,  28143,    100,    113,    126,  48506,\n",
       "         9091,  26737,  30005,  11018,  41693,    114,  14120,    100])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÏûÖÎ†• ÌÜ†ÌÅ∞Ïùò ÏµúÎåÄ ÏãúÌÄÄÏä§ Í∏∏Ïù¥\n",
    "MAX_LEN = 32\n",
    "\n",
    "# ÌÜ†ÌÅ∞ÏùÑ Ïà´Ïûê Ïù∏Îç±Ïä§Î°ú Î≥ÄÌôò\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# Î¨∏Ïû•ÏùÑ MAX_LEN Í∏∏Ïù¥Ïóê ÎßûÍ≤å ÏûêÎ•¥Í≥†, Î™®ÏûêÎûÄ Î∂ÄÎ∂ÑÏùÑ Ìå®Îî© 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ Ï¥àÍ∏∞Ìôî\n",
    "attention_masks = []\n",
    "\n",
    "# Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨Î•º Ìå®Îî©Ïù¥ ÏïÑÎãàÎ©¥ 1, Ìå®Îî©Ïù¥Î©¥ 0ÏúºÎ°ú ÏÑ§Ï†ï\n",
    "# Ìå®Îî© Î∂ÄÎ∂ÑÏùÄ BERT Î™®Îç∏ÏóêÏÑú Ïñ¥ÌÖêÏÖòÏùÑ ÏàòÌñâÌïòÏßÄ ÏïäÏïÑ ÏÜçÎèÑ Ìñ•ÏÉÅ\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [200, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-ec4ff02353df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                                                     \u001b[0msentiment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                                                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                                                                     test_size=0.3)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨Î•º ÌõàÎ†®ÏÖãÍ≥º Í≤ÄÏ¶ùÏÖãÏúºÎ°ú Î∂ÑÎ¶¨\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2125\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2127\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \"\"\"\n\u001b[0;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 256\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [200, 500]"
     ]
    }
   ],
   "source": [
    "# ÌõàÎ†®ÏÖãÍ≥º Í≤ÄÏ¶ùÏÖãÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    sentiment, \n",
    "                                                                                    random_state=2020, \n",
    "                                                                                    test_size=0.3)\n",
    "\n",
    "# Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨Î•º ÌõàÎ†®ÏÖãÍ≥º Í≤ÄÏ¶ùÏÖãÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2020, \n",
    "                                                       test_size=0.3)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞Î•º ÌååÏù¥ÌÜ†ÏπòÏùò ÌÖêÏÑúÎ°ú Î≥ÄÌôò\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])\n",
    "\n",
    "train_inputs.shape, train_labels.shape, train_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à\n",
    "batch_size = 3\n",
    "\n",
    "# ÌååÏù¥ÌÜ†ÏπòÏùò DataLoaderÎ°ú ÏûÖÎ†•, ÎßàÏä§ÌÅ¨, ÎùºÎ≤®ÏùÑ Î¨∂Ïñ¥ Îç∞Ïù¥ÌÑ∞ ÏÑ§Ï†ï\n",
    "# ÌïôÏäµÏãú Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à ÎßåÌÅº Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¥\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500    ÏïÑ ÎÇòÎäî ÏïÑÎî∞ÎßòÎßà Ïó∞Ï∂ú ÎÑàÎ¨¥ Ï¢ãÏïú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã...\n",
       "501                      ÎÇ¥Ïùº ÌïôÍµêÏóêÏÑú Ìè∞ ÏïàÎÇº ÏÉàÎüº Ïò§Ï°∞Ïò§ÏñµÎ™Ö #BTSBBMAs\n",
       "502                              ÎπÑÎã®Ïù¥ ÎÑò Ïù¥ÏÅòÍ≤å Í∑∏Î†§Ï£ºÏÖîÏÑú Í∏∞Î∂ÑÏù¥Ï¢ãÏäµÎãàÎã§\n",
       "503                                        ÌõÑÏúΩÌõÑÏúΩ Ïß±Ïù¥ÎªêÏßÑÏßúÏµúÍ≥†Ïïº\n",
       "504                                               Ïª§ÎØ∏ÏÖòÎßåÏÑ∏ \n",
       "                             ...                        \n",
       "696              ÎéÄÏù¥ ÏïàÏó¥Î†§ÏûàÏñ¥Ïöî„Ö†„Ö†„Ö†„Ö†„Ö†ÏßÑ„ÖàÏòÅÏù¥ Îß§Ïùº ÎΩëÍ≥† ÏûàÏñ¥Ïöî„Ö†„Ö†„Ö†„Ö†ÏñëÎèÑÎ∂ÄÌÉÅÎìúÎ†§Ïöî\n",
       "697                       Îã§ÎÖ§Ïù¥ Ìè¨Ïä§Ìä∏Ïûá Ï†úÎ∞ú ÎèåÎ†§Ï£ºÏÑ∏Ïöî„Öú„Öú„Öú Ï†úÎ∞ú Ï†úÎ∞úÏöî„Öú„Öú„Öú\n",
       "698    üíôüíôüíôÌîåÎØ∏Îäî ÏïàÏÇ¨Î©¥ ÌïòÎ£®ÎßåÏóê ÌíÄÎ¶ΩÎãàÎã§ Ï†úÎ∞ú ÏÇ¨ÏßÄÎßàÏÑ∏Ïöîüíô 7Îßå 7Ï≤úÏõê Ï£ºÍ≥† Í∞à Ïàò Ïûà...\n",
       "699    Ïú†Îã§. (ÎÑ§ Ìåî ÏïÑÎûòÎ°ú ÏÜê ÎëòÎü¨ Îì± Íº¨Ïò• ÎÅåÏñ¥ÏïàÍ≥†ÏÑú ÏûÖ ÎßûÎåÑ Ï±Ñ Îàà ÎÇ¥Î†§Í∞êÎäîÎã§) Ïú†...\n",
       "700                  Ï†ÄÏöî„Ö†„Ö† „Ö†„Ö†„Ö†„Ö†„Ö†Ï†úÎ∞ú ÏñëÎèÑÎ∂ÄÌÉÅÎìúÎ†§Ïöî„Ö†„Ö†„Ö† Íº≠ Í∞ÄÍ≥†Ïã∂Ïñ¥Ïöî„Ö†„Ö†„Ö†„Ö†\n",
       "Name: text, Length: 200, dtype: object"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÌÖåÏä§Ìä∏ ÏÖã\n",
    "test_sentences = df[\"text\"][500:700]\n",
    "test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] ÏïÑ ÎÇòÎäî ÏïÑÎî∞ÎßòÎßà Ïó∞Ï∂ú ÎÑàÎ¨¥ Ï¢ãÏïú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„ÖãÏïÑ ÏÑ±Ïö∞ÎãòÎèÑ ÎØ∏Ïπú Ïó∞Í∏∞Î†• „Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„ÖãüòÇüòÇüòÇ(5Î≤àÏß∏ ÎèåÎ†§Î≥¥ÎäîÏ§ë) https‚Ä¶ [SEP]',\n",
       " '[CLS] ÎÇ¥Ïùº ÌïôÍµêÏóêÏÑú Ìè∞ ÏïàÎÇº ÏÉàÎüº Ïò§Ï°∞Ïò§ÏñµÎ™Ö #BTSBBMAs [SEP]',\n",
       " '[CLS] ÎπÑÎã®Ïù¥ ÎÑò Ïù¥ÏÅòÍ≤å Í∑∏Î†§Ï£ºÏÖîÏÑú Í∏∞Î∂ÑÏù¥Ï¢ãÏäµÎãàÎã§ [SEP]',\n",
       " '[CLS] ÌõÑÏúΩÌõÑÏúΩ Ïß±Ïù¥ÎªêÏßÑÏßúÏµúÍ≥†Ïïº [SEP]',\n",
       " '[CLS] Ïª§ÎØ∏ÏÖòÎßåÏÑ∏  [SEP]',\n",
       " '[CLS] ÏÅòÏù¥ÏÅòÏù¥ÏÅòÏù¥Ïñ¥ÌÉùüòöüòö ÏÑ±ÌôòÏù¥Îûë Ìà¨Îã•Ìà¨Îã•ÎèÑ ÎÑàÎ¨¥ Í∑ÄÏó¨Ïõå....ÏóâÏóâ ÏòàÏÅòÎãà ÎÇ¥ÏÉàÍæ∏üò•üò• #ÎüΩÏò§Ìçº #ÏßÑÏÑ≠ #ÍπÄÏßÑÏÑ≠ #ÏÜåÎÖÑ24  [SEP]',\n",
       " '[CLS] Ïñ¥Ï†úÎèÑ Î≥¥Í≥† Ïã∂Í≥† Ïò§ÎäòÎèÑ Î≥¥Í≥† Ïã∂Í≥† ÎÇ¥ÏùºÎèÑ Î≥¥Í≥† Ïã∂Ïñ¥ [SEP]',\n",
       " '[CLS] Ïù¥ Í∏ÄÏóê Îã¨Î¶∞ ÎåìÍ∏Ä Î≥¥ÏÖ®ÎÇòÏöî„Öã„Öã„Öã„Öã„Öã ÎÑàÎ¨¥ ÏÜçÏãúÏõêüëç  [SEP]',\n",
       " '[CLS] ÎìúÎîîÏñ¥ ÎÇ¥ÏùºÎÇ¥ÏùºÎÇ¥Ïùº ! üíõ [SEP]',\n",
       " '[CLS] Ïö∞ÏôÄ Ïù¥Í≤å Î≤åÏç® 3ÎÖÑ Ï†Ñ ÏÇ¨ÏßÑÏù¥Îûò ÏÇ¨ÏßÑÏ≤© Ï†ïÎ¶¨ÌïòÎã§Í∞Ä ÎÜÄÎûêÎã§  [SEP]']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERTÏùò ÏûÖÎ†• ÌòïÏãùÏóê ÎßûÍ≤å Î≥ÄÌôò\n",
    "test_sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in test_sentences]\n",
    "test_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 3, 2, 2, 6, 2, 5, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2,\n",
       "       6, 1, 2, 5, 0, 0, 0, 0, 0, 2, 5, 5, 2, 2, 0, 1, 2, 5, 4, 4, 5, 5,\n",
       "       5, 4, 0, 4, 2, 5, 5, 5, 2, 0, 2, 4, 0, 0, 0, 0, 2, 2, 0, 4, 5, 0,\n",
       "       2, 2, 4, 2, 0, 2, 6, 4, 5, 5, 4, 0, 5, 4, 2, 4, 2, 2, 2, 3, 0, 6,\n",
       "       5, 5, 2, 5, 5, 4, 0, 4, 2, 2, 2, 2, 2, 2, 2, 5, 2, 0, 2, 5, 4, 2,\n",
       "       5, 2, 5, 6, 2, 2, 2, 2, 2, 6, 2, 0, 5, 2, 5, 2, 2, 5, 2, 5, 2, 5,\n",
       "       4, 6, 5, 0, 2, 2, 5, 2, 2, 6, 5, 2, 2, 2, 2, 3, 2, 5, 6, 3, 4, 5,\n",
       "       2, 2, 4, 4, 2, 2, 2, 2, 2, 4, 4, 5, 5, 6, 6, 6, 2, 5, 2, 5, 2, 2,\n",
       "       0, 5, 2, 5, 2, 2, 5, 6, 5, 2, 2, 5, 2, 2, 2, 2, 2, 2, 0, 5, 5, 5,\n",
       "       2, 5])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labelÏù¥ Î¨∏ÏûêÏù¥ÎØÄÎ°ú Ïà´ÏûêÍ∞íÏúºÎ°ú Î∞îÍøîÏ§òÏïºÌï® - LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "test_label_encoder = LabelEncoder()\n",
    "\n",
    "test_sentiment = test_label_encoder.fit_transform(df[\"sentiment\"][500:700])\n",
    "\n",
    "# label_encoder.classes_\n",
    "# label_encoder.inverse_transform(sentiment)\n",
    "\n",
    "# test_sentiment = test_sentiment.reshape(-1,1)\n",
    "test_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] ÏïÑ ÎÇòÎäî ÏïÑÎî∞ÎßòÎßà Ïó∞Ï∂ú ÎÑàÎ¨¥ Ï¢ãÏïú„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„ÖãÏïÑ ÏÑ±Ïö∞ÎãòÎèÑ ÎØ∏Ïπú Ïó∞Í∏∞Î†• „Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„Öã„ÖãüòÇüòÇüòÇ(5Î≤àÏß∏ ÎèåÎ†§Î≥¥ÎäîÏ§ë) https‚Ä¶ [SEP]\n",
      "['[CLS]', 'ÏïÑ', 'ÎÇòÎäî', 'ÏïÑ', '##Îî∞', '##Îßò', '##Îßà', 'Ïó∞', '##Ï∂ú', 'ÎÑà', '##Î¨¥', '[UNK]', 'ÏÑ±Ïö∞', '##Îãò', '##ÎèÑ', 'ÎØ∏', '##Ïπú', 'Ïó∞', '##Í∏∞', '##Î†•', '[UNK]', '(', '5', '##Î≤àÏß∏', 'Îèå', '##Î†§', '##Î≥¥', '##Îäî', '##Ï§ë', ')', 'https', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# BERTÏùò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Î¨∏Ïû•ÏùÑ ÌÜ†ÌÅ∞ÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
    "\n",
    "print (test_sentences[0])\n",
    "print (tokenized_texts[0])\n",
    "# UNKÎäî Î≠êÏßÄ? Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   101,   9519, 100585,   9519, 118825, 118911,  23811,   9568,\n",
       "        52363,   9004,  32537,    100,  68409, 108578,  12092,   9309,\n",
       "        55358,   9568,  12310,  28143,    100,    113,    126,  48506,\n",
       "         9091,  26737,  30005,  11018,  41693,    114,  14120,    100])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÏûÖÎ†• ÌÜ†ÌÅ∞Ïùò ÏµúÎåÄ ÏãúÌÄÄÏä§ Í∏∏Ïù¥ 128\n",
    "MAX_LEN = 32\n",
    "\n",
    "# ÌÜ†ÌÅ∞ÏùÑ Ïà´Ïûê Ïù∏Îç±Ïä§Î°ú Î≥ÄÌôò\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# Î¨∏Ïû•ÏùÑ MAX_LEN Í∏∏Ïù¥Ïóê ÎßûÍ≤å ÏûêÎ•¥Í≥†, Î™®ÏûêÎûÄ Î∂ÄÎ∂ÑÏùÑ Ìå®Îî© 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ Ï¥àÍ∏∞Ìôî\n",
    "attention_masks = []\n",
    "\n",
    "# Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨Î•º Ìå®Îî©Ïù¥ ÏïÑÎãàÎ©¥ 1, Ìå®Îî©Ïù¥Î©¥ 0ÏúºÎ°ú ÏÑ§Ï†ï\n",
    "# Ìå®Îî© Î∂ÄÎ∂ÑÏùÄ BERT Î™®Îç∏ÏóêÏÑú Ïñ¥ÌÖêÏÖòÏùÑ ÏàòÌñâÌïòÏßÄ ÏïäÏïÑ ÏÜçÎèÑ Ìñ•ÏÉÅ\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   101,   9519, 100585,   9519, 118825, 118911,  23811,   9568,  52363,\n",
      "          9004,  32537,    100,  68409, 108578,  12092,   9309,  55358,   9568,\n",
      "         12310,  28143,    100,    113,    126,  48506,   9091,  26737,  30005,\n",
      "         11018,  41693,    114,  14120,    100], dtype=torch.int32)\n",
      "tensor(2, dtype=torch.int32)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞Î•º ÌååÏù¥ÌÜ†ÏπòÏùò ÌÖêÏÑúÎ°ú Î≥ÄÌôò\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(test_sentiment)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_inputs[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à\n",
    "batch_size = 3\n",
    "\n",
    "# ÌååÏù¥ÌÜ†ÏπòÏùò DataLoaderÎ°ú ÏûÖÎ†•, ÎßàÏä§ÌÅ¨, ÎùºÎ≤®ÏùÑ Î¨∂Ïñ¥ Îç∞Ïù¥ÌÑ∞ ÏÑ§Ï†ï\n",
    "# ÌïôÏäµÏãú Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à ÎßåÌÅº Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¥\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-8c2f2159c4e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# GPU ÎîîÎ∞îÏù¥Ïä§ Ïù¥Î¶Ñ Íµ¨Ìï®\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# GPU ÎîîÎ∞îÏù¥Ïä§ Ïù¥Î¶Ñ Í≤ÄÏÇ¨\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79601b6aa9dc466b811db9a92bde90cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa950fa740b64e58897c9103ae75fd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Î∂ÑÎ•òÎ•º ÏúÑÌïú BERT Î™®Îç∏ ÏÉùÏÑ±\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=7)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÑ§Ï†ï\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # ÌïôÏäµÎ•†\n",
    "                  eps = 1e-8 # 0ÏúºÎ°ú ÎÇòÎàÑÎäî Í≤ÉÏùÑ Î∞©ÏßÄÌïòÍ∏∞ ÏúÑÌïú epsilon Í∞í\n",
    "                )\n",
    "\n",
    "# ÏóêÌè≠Ïàò\n",
    "epochs = 4\n",
    "\n",
    "# Ï¥ù ÌõàÎ†® Ïä§ÌÖù : Î∞∞ÏπòÎ∞òÎ≥µ ÌöüÏàò * ÏóêÌè≠\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# ÌïôÏäµÎ•†ÏùÑ Ï°∞Í∏àÏî© Í∞êÏÜåÏãúÌÇ§Îäî Ïä§ÏºÄÏ§ÑÎü¨ ÏÉùÏÑ±\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏãúÍ∞Ñ ÌëúÏãú Ìï®Ïàò\n",
    "def format_time(elapsed):\n",
    "\n",
    "    # Î∞òÏò¨Î¶º\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # hh:mm:ssÏúºÎ°ú ÌòïÌÉú Î≥ÄÍ≤Ω\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "tensor([5, 2, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-165ff80c3ab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m                         \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                         labels=b_labels)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Î°úÏä§ Íµ¨Ìï®\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1324\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1326\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m         )\n\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         embedding_output = self.embeddings(\n\u001b[1;32m--> 825\u001b[1;33m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m         )\n\u001b[0;32m    827\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m         return F.embedding(\n\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "# Ïû¨ÌòÑÏùÑ ÏúÑÌï¥ ÎûúÎç§ÏãúÎìú Í≥†Ï†ï\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï¥àÍ∏∞Ìôî\n",
    "model.zero_grad()\n",
    "\n",
    "# ÏóêÌè≠ÎßåÌÅº Î∞òÎ≥µ\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # ÏãúÏûë ÏãúÍ∞Ñ ÏÑ§Ï†ï\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Î°úÏä§ Ï¥àÍ∏∞Ìôî\n",
    "    total_loss = 0\n",
    "\n",
    "    # ÌõàÎ†®Î™®ÎìúÎ°ú Î≥ÄÍ≤Ω\n",
    "    model.train()\n",
    "        \n",
    "    # Îç∞Ïù¥ÌÑ∞Î°úÎçîÏóêÏÑú Î∞∞ÏπòÎßåÌÅº Î∞òÎ≥µÌïòÏó¨ Í∞ÄÏ†∏Ïò¥\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Í≤ΩÍ≥º Ï†ïÎ≥¥ ÌëúÏãú\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Î∞∞ÏπòÎ•º GPUÏóê ÎÑ£Ïùå\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Î∞∞ÏπòÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "        b_labels = b_labels.type(torch.LongTensor)\n",
    "\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "\n",
    "        print(b_labels)\n",
    "\n",
    "        # Forward ÏàòÌñâ                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # Î°úÏä§ Íµ¨Ìï®\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Ï¥ù Î°úÏä§ Í≥ÑÏÇ∞\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward ÏàòÌñâÏúºÎ°ú Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞\n",
    "        loss.backward()\n",
    "\n",
    "        # Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¥Î¶¨Ìïë\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Í∑∏ÎûòÎîîÏñ∏Ìä∏Î•º ÌÜµÌï¥ Í∞ÄÏ§ëÏπò ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "        optimizer.step()\n",
    "\n",
    "        # Ïä§ÏºÄÏ§ÑÎü¨Î°ú ÌïôÏäµÎ•† Í∞êÏÜå\n",
    "        scheduler.step()\n",
    "\n",
    "        # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Ï¥àÍ∏∞Ìôî\n",
    "        model.zero_grad()\n",
    "\n",
    "    # ÌèâÍ∑† Î°úÏä§ Í≥ÑÏÇ∞\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #ÏãúÏûë ÏãúÍ∞Ñ ÏÑ§Ï†ï\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ÌèâÍ∞ÄÎ™®ÎìúÎ°ú Î≥ÄÍ≤Ω\n",
    "    model.eval()\n",
    "\n",
    "    # Î≥ÄÏàò Ï¥àÍ∏∞Ìôî\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞Î°úÎçîÏóêÏÑú Î∞∞ÏπòÎßåÌÅº Î∞òÎ≥µÌïòÏó¨ Í∞ÄÏ†∏Ïò¥\n",
    "    for batch in validation_dataloader:\n",
    "        # Î∞∞ÏπòÎ•º GPUÏóê ÎÑ£Ïùå\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Î∞∞ÏπòÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        b_input_ids = b_input_ids.type(torch.LongTensor)\n",
    "        b_input_mask = b_input_mask.type(torch.LongTensor)\n",
    "        b_labels = b_labels.type(torch.LongTensor)\n",
    "\n",
    "        b_input_ids = b_input_ids.to(device)\n",
    "        b_input_mask = b_input_mask.to(device)\n",
    "        b_labels = b_labels.to(device)\n",
    "        \n",
    "        # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞ ÏïàÌï®\n",
    "        with torch.no_grad():     \n",
    "            # Forward ÏàòÌñâ\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Î°úÏä§ Íµ¨Ìï®\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # CPUÎ°ú Îç∞Ïù¥ÌÑ∞ Ïù¥Îèô\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Ï∂úÎ†• Î°úÏßìÍ≥º ÎùºÎ≤®ÏùÑ ÎπÑÍµêÌïòÏó¨ Ï†ïÌôïÎèÑ Í≥ÑÏÇ∞\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BertForSequenceClassification' from 'transformers' (C:\\Users\\gmlrn\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-12cfae9ba62f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BertForSequenceClassification' from 'transformers' (C:\\Users\\gmlrn\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1650 Ti\n"
     ]
    }
   ],
   "source": [
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dir ÏÑ§Ï†ï(Ï†ÄÏû•Îêú Î™®Îç∏Ïù¥ ÏûàÎäî Í≤ΩÎ°ú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../../../sentiment_analysis_bert/BERT Fine-Tuning/model_save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERTÏùò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ°ú Î¨∏Ïû•ÏùÑ ÌÜ†ÌÅ∞ÏúºÎ°ú Î∂ÑÎ¶¨\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # ÏûÖÎ†• ÌÜ†ÌÅ∞Ïùò ÏµúÎåÄ ÏãúÌÄÄÏä§ Í∏∏Ïù¥\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # ÌÜ†ÌÅ∞ÏùÑ Ïà´Ïûê Ïù∏Îç±Ïä§Î°ú Î≥ÄÌôò\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # Î¨∏Ïû•ÏùÑ MAX_LEN Í∏∏Ïù¥Ïóê ÎßûÍ≤å ÏûêÎ•¥Í≥†, Î™®ÏûêÎûÄ Î∂ÄÎ∂ÑÏùÑ Ìå®Îî© 0ÏúºÎ°ú Ï±ÑÏõÄ\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ Ï¥àÍ∏∞Ìôî\n",
    "    attention_masks = []\n",
    "\n",
    "    # Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨Î•º Ìå®Îî©Ïù¥ ÏïÑÎãàÎ©¥ 1, Ìå®Îî©Ïù¥Î©¥ 0ÏúºÎ°ú ÏÑ§Ï†ï\n",
    "    # Ìå®Îî© Î∂ÄÎ∂ÑÏùÄ BERT Î™®Îç∏ÏóêÏÑú Ïñ¥ÌÖêÏÖòÏùÑ ÏàòÌñâÌïòÏßÄ ÏïäÏïÑ ÏÜçÎèÑ Ìñ•ÏÉÅ\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞Î•º ÌååÏù¥ÌÜ†ÏπòÏùò ÌÖêÏÑúÎ°ú Î≥ÄÌôò\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î¨∏Ïû• ÌÖåÏä§Ìä∏\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # ÌèâÍ∞ÄÎ™®ÎìúÎ°ú Î≥ÄÍ≤Ω\n",
    "    model.eval()\n",
    "\n",
    "    # Î¨∏Ïû•ÏùÑ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î°ú Î≥ÄÌôò\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "    \n",
    " \n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    masks = masks.type(torch.LongTensor)\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞Î•º GPUÏóê ÎÑ£Ïùå\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    # Í∑∏ÎûòÎîîÏñ∏Ìä∏ Í≥ÑÏÇ∞ ÏïàÌï®\n",
    "    with torch.no_grad():     \n",
    "        # Forward ÏàòÌñâ\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # Î°úÏä§ Íµ¨Ìï®\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPUÎ°ú Îç∞Ïù¥ÌÑ∞ Ïù¥Îèô\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dic = {\n",
    "    0 : 'anger',\n",
    "    1 : 'fear',\n",
    "    2 : 'joy',\n",
    "    3 : 'love',\n",
    "    4 : 'neutral',\n",
    "    5 : 'sadness',\n",
    "    6 : 'surprise',\n",
    "}\n",
    "\n",
    "# ['anger', 'fear', 'joy', 'love', 'neutral', 'sadness', 'surprise']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38248247 -1.7115413   3.7598755  -0.3330785  -0.04192548 -0.45470026\n",
      "  -0.72179   ]]\n",
      "joy\n"
     ]
    }
   ],
   "source": [
    "logits = test_sentences(['ÏµúÌÉúÏÑ±ÏùòÌïúÍµ≠ÏÇ¨Î•ºÎì§Ïñ¥ÏãúÌóòÍ≤åÌï©Í≤©Ìï¥ÏÑúÍ∏∞ÏÅòÎã§'])\n",
    "\n",
    "print(logits)\n",
    "print(sentiment_dic[np.argmax(logits)])\n",
    "\n",
    "# label_encoder.inverse_transform(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-15 15:30:51.439330: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\n",
      "2020-09-15 15:30:51.439914: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We will use the GPU: GeForce GTX 1650 Ti\n",
      "[[-0.34741524 -0.08288456  0.6051411   0.708734    0.007935    0.09675492\n",
      "  -0.6876814 ]]\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "!python model_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "!python torch_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'zeros'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\AI_SCHOOL\\WarmingUp\\feelm\\save_load_model\\torch.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgpu_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpu_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'zeros'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
